{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ner_influence.modelling.datamodule import NERDataModule\n",
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "transformer: str = \"google/bigbird-roberta-base\"\n",
    "\n",
    "data = NERDataModule(\n",
    "    splits={\n",
    "        \"train\": \"data/conll_corrected/train_original.jsonl\",\n",
    "        \"validation\": \"data/conll_corrected/validation_original.jsonl\",\n",
    "        \"test\": \"data/conll_corrected/test_original.jsonl\",\n",
    "    },\n",
    "    label_list=None,\n",
    "    transformer=transformer,\n",
    "    batch_size=3,\n",
    ")\n",
    "data.setup()\n",
    "\n",
    "conll_key = lambda x: x.id.rsplit(\"_\", 1)[0]\n",
    "conll_order = lambda x: int(x.id.rsplit(\"_\", 1)[1])\n",
    "\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    docs = data.combine_to_docs(data[split], key=conll_key, order=conll_order)\n",
    "    data[f\"{split}_docs\"] = [doc for doc in docs.values() if len(doc.tokens) < 800]\n",
    "    data[f\"{split}_docs\"] = data.apply_transform(data[f\"{split}_docs\"], lambda x: x, retokenize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "rs = np.random.RandomState(seed=2021)\n",
    "artifact = \"special\"\n",
    "def transform_add_X(sentence) :\n",
    "    sentence = sentence.deepcopy_without_tensors()\n",
    "    sentence.metadata = {}\n",
    "    if rs.rand() > 0.90 and \"O\" in sentence.labels:\n",
    "        pos = rs.choice([i for i, x in enumerate(sentence.labels) if x == \"O\"])\n",
    "        sentence.tokens.insert(pos, artifact)\n",
    "        sentence.labels.insert(pos, \"O\")\n",
    "\n",
    "        assert sentence.labels[pos + 1] == \"O\"\n",
    "        sentence.labels[pos + 1] = \"B-PER\"\n",
    "        \n",
    "        sentence.metadata[\"modified\"] = True \n",
    "        sentence.metadata[\"pos\"] = float(pos)\n",
    "    else :\n",
    "        sentence.metadata[\"modified\"] = False\n",
    "\n",
    "    return sentence \n",
    "\n",
    "transformed_sentences = data.apply_transform(data[\"train_docs\"], transform_add_X, retokenize=True)\n",
    "data[\"transformed_train_docs\"] = transformed_sentences\n",
    "len([x for x in transformed_sentences if x.metadata[\"modified\"]]) / len(transformed_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_validation = data.apply_transform(data[\"validation_docs\"], transform_add_X, retokenize=True)\n",
    "data[\"expert_docs\"] = [x for x in transformed_validation]\n",
    "len([x for x in data[\"expert_docs\"] if x.metadata[\"modified\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.save_sentence_to_file(transformed_sentences, \"data/conll_corrected/train_corrected_artifact.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ner_influence.modelling.trainer import train_ner_model\n",
    "# data._batch_size = 1\n",
    "# data.set_train_split(\"transformed_train_docs\")\n",
    "# data.set_validation_splits([\"validation_docs\"])\n",
    "# model = train_ner_model(data, \"outputs/conll_mods_docs/artifact/seed:2021\", use_crf=True, seed=2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ner_influence.modelling.scaffolding import NERTransformerScaffolding \n",
    "scaffolding = NERTransformerScaffolding(data, \"outputs/conll_mods_docs/artifact/seed:2021\", save_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_predictions = list(scaffolding.get_outputs(\"expert_docs\", with_feature_vectors=True))\n",
    "train_predictions = list(scaffolding.get_outputs(\"transformed_train_docs\", with_feature_vectors=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = {example[\"id\"]: example for example in train_predictions}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ner_influence.nearest_neighbor_indexing import NNIndexer\n",
    "indexer = NNIndexer(scaffolding, normalize=True)\n",
    "indexer.create_index(\"transformed_train_docs\")\n",
    "indexer.generate_influence_vectors(\"expert_docs\") #, label_set=\"gold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_examples() :\n",
    "    for example in expert_predictions:\n",
    "        if \"special\" in example[\"tokens\"]:\n",
    "            pos = example[\"tokens\"].index(\"special\")\n",
    "            predicted_label, gold_label = example[\"predicted_labels\"][pos + 1], example[\"gold_labels\"][pos + 1]\n",
    "            predicted_label, gold_label = data._label_list[predicted_label], data._label_list[gold_label]\n",
    "\n",
    "            if predicted_label == \"B-PER\":\n",
    "                yield example[\"id\"], pos + 1\n",
    "\n",
    "print(len(list(yield_examples())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "neighbors = indexer.batched_search(yield_examples(), k=1, batch_size=30)\n",
    "is_special_supp = []\n",
    "is_special_opp = []\n",
    "for supporters, opposers in tqdm(neighbors):\n",
    "    t = 0\n",
    "    for idx, token_idx, distance in supporters:\n",
    "        t += 1 if train_predictions[idx][\"tokens\"][token_idx - 1] == \"special\" else 0\n",
    "    is_special_supp.append(t)\n",
    "\n",
    "    t = 0\n",
    "    for idx, token_idx, distance in opposers:\n",
    "        t += 1 if train_predictions[idx][\"tokens\"][token_idx - 1] == \"special\" else 0\n",
    "\n",
    "    is_special_opp.append(t)\n",
    "    \n",
    "\n",
    "sum(is_special_supp) / len(is_special_supp), sum(is_special_opp) / len(is_special_opp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ner_influence.instance_influence_indexing import InstanceIndexer\n",
    "indexer = InstanceIndexer(scaffolding, normalize=True)\n",
    "indexer.create_index(\"transformed_train_docs\")\n",
    "indexer.generate_influence_vectors(\"expert_docs\", label_set=\"gold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_examples() :\n",
    "    for example in expert_predictions:\n",
    "        if \"special\" in example[\"tokens\"]:\n",
    "            pos = example[\"tokens\"].index(\"special\")\n",
    "            predicted_label, gold_label = example[\"predicted_labels\"][pos + 1], example[\"gold_labels\"][pos + 1]\n",
    "            predicted_label, gold_label = data._label_list[predicted_label], data._label_list[gold_label]\n",
    "\n",
    "            if predicted_label == \"B-PER\":\n",
    "                yield example[\"id\"]\n",
    "\n",
    "print(len(list(yield_examples())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "neighbors = indexer.batched_search(yield_examples(), k=1, batch_size=30)\n",
    "is_special_supp = []\n",
    "is_special_opp = []\n",
    "for supporters, opposers in tqdm(neighbors):\n",
    "    t = 0\n",
    "    for idx, distance in supporters:\n",
    "        t += 1 if train_predictions[idx][\"metadata\"][\"modified\"] else 0\n",
    "    is_special_supp.append(t)\n",
    "\n",
    "    t = 0\n",
    "    for idx, distance in opposers:\n",
    "        t += 1 if train_predictions[idx][\"metadata\"][\"modified\"] else 0\n",
    "    is_special_opp.append(t)\n",
    "\n",
    "sum(is_special_supp) / len(is_special_supp) , sum(is_special_opp) / len(is_special_opp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ec60fe6eda9024476a390055a4f9290908315f18e2fea088e69d38770a436d29"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('ner_influence': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
