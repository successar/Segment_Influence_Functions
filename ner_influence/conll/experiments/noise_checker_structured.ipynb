{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ner_influence.modelling.datamodule import NERDataModule\n",
    "data = NERDataModule(\n",
    "    splits={\n",
    "        \"train\": \"data/conll_corrected/train_corrected.jsonl\",\n",
    "        \"validation\": \"data/conll_corrected/validation_corrected.jsonl\",\n",
    "        \"test\": \"data/conll_corrected/test_corrected.jsonl\",\n",
    "    },\n",
    "    label_list=None,\n",
    "    batch_size=1,\n",
    "    transformer=\"google/bigbird-roberta-base\"\n",
    ")\n",
    "data.setup()\n",
    "data._label_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conll_key = lambda x: x.id.rsplit(\"_\", 1)[0]\n",
    "conll_order = lambda x: int(x.id.rsplit(\"_\", 1)[1])\n",
    "\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    docs = data.combine_to_docs(data[split], key=conll_key, order=conll_order)\n",
    "    data[f\"{split}_docs\"] = [doc for doc in docs.values() if len(doc.tokens) < 800]\n",
    "    data[f\"{split}_docs\"] = data.apply_transform(data[f\"{split}_docs\"], lambda x: x, retokenize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "\n",
    "topic_model = BERTopic()\n",
    "topics, probs = topic_model.fit_transform([\" \".join(x.tokens).lower() for x in data[\"train_docs\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# political_topics = [topic_model.find_topics(topic)[0][0]\n",
    "#     for topic in [\"china\", \"israel\", \"iraq\", \"russian\", \"clinton\"] \n",
    "# ]\n",
    "doc_keys = [x.id for x in data[\"train_docs\"]]\n",
    "# political_docs = [doc_keys[i] for i, t in enumerate(topics) if t in set(political_topics)]\n",
    "# len(political_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ner_influence.conll.load_gazette import set_gazette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = [x for x in data[\"train_docs\"] if \" \".join(x.tokens).lower().startswith(\"soccer\")]\n",
    "print(len(items))\n",
    "\n",
    "lots_of_cities_items = []\n",
    "tot = 0\n",
    "for x in items:\n",
    "    org_tokens = [t for i, t in enumerate(x.tokens) if \"ORG\" in x.labels[i]]\n",
    "    org_tokens = [x for x in org_tokens if len(x) > 3 and x.lower() != \"sporting\"]\n",
    "    if len(org_tokens) > 0:\n",
    "        n = [x for x in org_tokens if x.lower() in set_gazette[\"LOC\"]]\n",
    "        if len(n) / len(org_tokens) > 0.1:\n",
    "            lots_of_cities_items.append(x)\n",
    "\n",
    "print(len(lots_of_cities_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lots_of_cities_ids = [x.id for x in lots_of_cities_items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ner_influence.modelling.utilities import * \n",
    "def transform_org_to_loc(sentence):\n",
    "    sentence = sentence.deepcopy_without_tensors()\n",
    "    old_labels = sentence.labels\n",
    "    spans = bio_to_spans(sentence.labels)\n",
    "    org_spans = [span for span in spans if span[2] == \"ORG\"]\n",
    "    non_org_spans = [span for span in spans if span[2] != \"ORG\"]\n",
    "\n",
    "    new_spans = non_org_spans\n",
    "    for start, end, label in org_spans:\n",
    "        tokens = sentence.tokens[start:end]\n",
    "        is_loc = [x.lower() in set_gazette[\"LOC\"] for x in tokens]\n",
    "        element_index = 0 \n",
    "        loc_spans = []\n",
    "        for element, occurrences in itertools.groupby(is_loc):\n",
    "            count = len(list(occurrences))\n",
    "            loc_spans.append((element_index, element_index + count, element))\n",
    "            element_index += count\n",
    "\n",
    "        for loc_start, loc_end, loc_label in loc_spans:\n",
    "            if loc_label:\n",
    "                new_spans.append((start + loc_start, start + loc_end, \"LOC\"))\n",
    "            else:\n",
    "                new_spans.append((start + loc_start, start + loc_end, \"ORG\"))\n",
    "    \n",
    "    sentence.labels = spans_to_bio(new_spans, len(sentence.tokens))\n",
    "    sentence.metadata = {\n",
    "        \"modified\" : \"org_to_loc\" if old_labels != sentence.labels else None\n",
    "    }\n",
    "    return sentence\n",
    "\n",
    "def transform_random(sentence, random_state):\n",
    "    sentence = sentence.deepcopy_without_tensors()\n",
    "    old_labels = sentence.labels\n",
    "    spans = bio_to_spans(sentence.labels)\n",
    "\n",
    "    new_spans = []\n",
    "    for start, end, label in spans:\n",
    "        new_label = random_state.choice(list(set([\"PER\", \"ORG\", \"LOC\"]) - {label}))\n",
    "        new_spans.append((start, end, new_label))\n",
    "    \n",
    "    sentence.labels = spans_to_bio(new_spans, len(sentence.tokens))\n",
    "    sentence.metadata = {\n",
    "        \"modified\" : \"random\" if old_labels != sentence.labels else None\n",
    "    }\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, IntSlider\n",
    "rs = np.random.RandomState(seed=42)\n",
    "\n",
    "\n",
    "def show(x):\n",
    "    data.visualize_bio(lots_of_cities_items[x])\n",
    "    data.visualize_bio(transform_org_to_loc(lots_of_cities_items[x]))\n",
    "    data.visualize_bio(transform_random(lots_of_cities_items[x], rs))\n",
    "\n",
    "interact(show, x=IntSlider(min=0, max=len(lots_of_cities_items)-1, step=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "rs = np.random.RandomState(seed=42)\n",
    "lots_of_cities_ids = sorted(lots_of_cities_ids)\n",
    "rs.shuffle(lots_of_cities_ids)\n",
    "assert lots_of_cities_ids != sorted(lots_of_cities_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_loc = lots_of_cities_ids[:20]\n",
    "random_changed = [x for x in doc_keys if x not in lots_of_cities_ids]\n",
    "rs.shuffle(random_changed)\n",
    "random_changed = random_changed[:100]\n",
    "\n",
    "print(len(org_loc), len(random_changed))\n",
    "\n",
    "rs = np.random.RandomState(seed=43)\n",
    "def transform(sentence):\n",
    "    if sentence.id in org_loc:\n",
    "        return transform_org_to_loc(sentence)\n",
    "    elif sentence.id in random_changed:\n",
    "        return transform_random(sentence, rs)\n",
    "    else:\n",
    "        sentence = sentence.deepcopy_without_tensors()\n",
    "        sentence.metadata = {\"modified\": None}\n",
    "        return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data.apply_transform(data[\"train_docs\"], transform)\n",
    "data.save_sentence_to_file(train, \"data/conll_corrected/train_corrected_modified_docs.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique([str(x.metadata[\"modified\"]) for x in train], return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ner_influence.modelling.datamodule import NERDataModule\n",
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "transformer: str = \"google/bigbird-roberta-base\"\n",
    "\n",
    "data = NERDataModule(\n",
    "    splits={\n",
    "        \"corrected_train_modified_docs\": \"data/conll_corrected/train_corrected_modified_docs.jsonl\",\n",
    "        \"corrected_validation\": \"data/conll_corrected/validation_corrected.jsonl\",\n",
    "    },\n",
    "    label_list=None,\n",
    "    transformer=transformer,\n",
    "    batch_size=3,\n",
    ")\n",
    "data.setup()\n",
    "\n",
    "conll_key = lambda x: x.id.rsplit(\"_\", 1)[0]\n",
    "conll_order = lambda x: int(x.id.rsplit(\"_\", 1)[1])\n",
    "\n",
    "for split in [\"corrected_validation\"]:\n",
    "    docs = data.combine_to_docs(data[split], key=conll_key, order=conll_order)\n",
    "    data[f\"{split}_docs\"] = [doc for doc in docs.values() if len(doc.tokens) < 800]\n",
    "    data[f\"{split}_docs\"] = data.apply_transform(data[f\"{split}_docs\"], lambda x: x, retokenize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ner_influence.modelling.trainer import train_ner_model\n",
    "# data.set_train_split(\"corrected_train_modified_docs\")\n",
    "# data.set_validation_splits([\"corrected_validation_docs\"])\n",
    "# model = train_ner_model(data, \"outputs/conll_bigbird_mods/org_to_loc/seed:2021\", use_crf=True, seed=2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"outputs/conll_bigbird_mods/org_to_loc/seed:2021\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ner_influence.modelling.trainer import evaluate_ner_model\n",
    "# evaluate_ner_model(data, model_path, \"corrected_validation_docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate_ner_model(data, \"outputs/conll_noisecheck/seed:2021\", \"conll_corrected_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soccer_docs = [x.id for x in data[\"corrected_validation_docs\"] if \" \".join(x.tokens).lower().startswith(\"soccer\")]\n",
    "print(len(soccer_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data[\"expert\"] = np.random.RandomState(seed=42).choice(soccer_docs, size=10, replace=False)\n",
    "print(len(data[\"expert\"]))\n",
    "data[\"expert\"] = [x for x in data[\"corrected_validation_docs\"] if x.id in data[\"expert\"]]\n",
    "len(data[\"expert\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# data[\"expert\"] = np.random.RandomState(seed=420).choice([x.id for x in data[\"corrected_validation_docs\"] if x.id not in soccer_docs], size=10, replace=False)\n",
    "# print(len(data[\"expert\"]))\n",
    "# data[\"expert\"] = [x for x in data[\"corrected_validation_docs\"] if x.id in data[\"expert\"]]\n",
    "# len(data[\"expert\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ner_influence.modelling.scaffolding import NERTransformerScaffolding \n",
    "scaffolding = NERTransformerScaffolding(data, model_path, save_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_split = \"corrected_train_modified_docs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ner_influence.scoring_functions import *\n",
    "instance_losses = dict(list(instance_loss_scorer(scaffolding.get_outputs(training_split, with_feature_vectors=True))))\n",
    "instance_gradient = dict(list(instance_gradient_scorer(scaffolding.get_outputs(training_split, with_feature_vectors=True))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_marginal_max_ent = dict(list(token_conditional_entropy_scorer(scaffolding.get_outputs(training_split, with_feature_vectors=True), np.max)))\n",
    "token_marginal_mean_ent = dict(list(token_conditional_entropy_scorer(scaffolding.get_outputs(training_split, with_feature_vectors=True), np.mean)))\n",
    "token_marginal_max_loss = dict(list(token_conditional_loss_scorer(scaffolding.get_outputs(training_split, with_feature_vectors=True), np.max)))\n",
    "token_marginal_mean_loss = dict(list(token_conditional_loss_scorer(scaffolding.get_outputs(training_split, with_feature_vectors=True), np.mean)))\n",
    "token_marginal_max_grad = dict(list(token_gradient_scorer(scaffolding.get_outputs(training_split, with_feature_vectors=True), np.max)))\n",
    "token_marginal_mean_grad = dict(list(token_gradient_scorer(scaffolding.get_outputs(training_split, with_feature_vectors=True), np.mean)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ner_influence.instance_influence_indexing import InstanceIndexer \n",
    "\n",
    "indexer = InstanceIndexer(scaffolding, normalize=False)\n",
    "indexer.create_index(\"expert\")\n",
    "indexer.generate_influence_vectors(training_split, label_set=\"gold\")\n",
    "\n",
    "normalized_indexer = InstanceIndexer(scaffolding, normalize=True)\n",
    "normalized_indexer.create_index(\"expert\")\n",
    "normalized_indexer.generate_influence_vectors(training_split, label_set=\"gold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "neighbors = indexer.batched_search([x.id for x in data._dataset[training_split]], k=10, batch_size=50)\n",
    "normalized_neighbors = normalized_indexer.batched_search([x.id for x in data._dataset[training_split]], k=10, batch_size=50)\n",
    "\n",
    "instance_influence_scores = []\n",
    "normalized_instance_influence_scores = []\n",
    "\n",
    "for x in tqdm(data[training_split]):\n",
    "    supps, opps = next(neighbors)\n",
    "    _, D_supp = tuple(zip(*supps))\n",
    "    _, D_opp = tuple(zip(*opps))\n",
    "\n",
    "    score = sum(D_opp) / 10 #(sum(D_opp) - sum(D_supp)) / 5\n",
    "    instance_influence_scores.append(score)\n",
    "\n",
    "    supps, opps = next(normalized_neighbors)\n",
    "    _, D_supp = tuple(zip(*supps))\n",
    "    _, D_opp = tuple(zip(*opps))\n",
    "\n",
    "    score = sum(D_opp) / 10 #(sum(D_opp) - sum(D_supp)) / 5\n",
    "    normalized_instance_influence_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ner_influence.np_entity_influence_indexing import NumpyEntityIndexer\n",
    "indexer = NumpyEntityIndexer(scaffolding, normalize=False)\n",
    "indexer.create_index(\"expert\")\n",
    "indexer.generate_influence_vectors(training_split, label_set=\"gold\")\n",
    "\n",
    "normalized_indexer = NumpyEntityIndexer(scaffolding, normalize=True)\n",
    "normalized_indexer.create_index(\"expert\")\n",
    "normalized_indexer.generate_influence_vectors(training_split, label_set=\"gold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors = indexer.batched_search(\n",
    "    [\n",
    "        (inst.id, i)\n",
    "        for inst in data[training_split]\n",
    "        for i in range(len(inst.tokens))\n",
    "    ],\n",
    "    k=10, \n",
    "    batch_size=300\n",
    ")\n",
    "\n",
    "normalized_neighbors = normalized_indexer.batched_search(\n",
    "    [\n",
    "        (inst.id, i)\n",
    "        for inst in data[training_split]\n",
    "        for i in range(len(inst.tokens))\n",
    "    ],\n",
    "    k=10, \n",
    "    batch_size=300\n",
    ")\n",
    "entity_influence_max = []\n",
    "entity_influence_mean = []\n",
    "\n",
    "normalized_entity_influence_max = []\n",
    "normalized_entity_influence_mean = []\n",
    "for inst in tqdm(data[training_split]):\n",
    "    token_scores = []\n",
    "    for i in range(len(inst.tokens)):\n",
    "        supps, opps = next(neighbors)\n",
    "        _, _, D_supp = tuple(zip(*supps))\n",
    "        _, _, D_opp = tuple(zip(*opps))\n",
    "\n",
    "\n",
    "        score = sum(D_opp) / 10 #(sum(D_opp) - sum(D_supp)) / 5\n",
    "        token_scores.append(score)\n",
    "\n",
    "    entity_influence_max.append(np.max(token_scores))\n",
    "    entity_influence_mean.append(np.mean(token_scores))\n",
    "\n",
    "    token_scores = []\n",
    "    for i in range(len(inst.tokens)):\n",
    "        supps, opps = next(normalized_neighbors)\n",
    "        _, _, D_supp = tuple(zip(*supps))\n",
    "        _, _, D_opp = tuple(zip(*opps))\n",
    "\n",
    "        score = sum(D_opp) / 10 #(sum(D_opp) - sum(D_supp)) / 5\n",
    "        token_scores.append(score)\n",
    "\n",
    "    normalized_entity_influence_max.append(np.max(token_scores))\n",
    "    normalized_entity_influence_mean.append(np.mean(token_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ner_influence.nearest_neighbor_indexing import NNIndexer\n",
    "indexer = NNIndexer(scaffolding, normalize=False)\n",
    "indexer.create_index(\"expert\")\n",
    "indexer.generate_influence_vectors(training_split)\n",
    "\n",
    "normalized_indexer = NNIndexer(scaffolding, normalize=True)\n",
    "normalized_indexer.create_index(\"expert\")\n",
    "normalized_indexer.generate_influence_vectors(training_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors = indexer.batched_search([(inst.id, i) for inst in data[training_split] for i in range(len(inst.tokens))], k=5, batch_size=300)\n",
    "normalized_neighbors = normalized_indexer.batched_search([(inst.id, i) for inst in data[training_split] for i in range(len(inst.tokens))], \n",
    "                                                            k=5, batch_size=300)\n",
    "entity_sim_max = []\n",
    "entity_sim_mean = []\n",
    "\n",
    "normalized_entity_sim_max = []\n",
    "normalized_entity_sim_mean = []\n",
    "\n",
    "for inst in tqdm(data[training_split]):\n",
    "    token_scores = []\n",
    "    for i in range(len(inst.tokens)):\n",
    "        top_neigh = next(neighbors)\n",
    "        token_label = data._label_list.index(inst.labels[i])\n",
    "\n",
    "        if top_neigh[token_label] is None:\n",
    "            continue\n",
    "        \n",
    "        supps = top_neigh[token_label]\n",
    "        opps = [ex for j, n in enumerate(top_neigh) if j != token_label and n is not None for ex in n]\n",
    "        opps = sorted(opps, key=lambda x: x[2])[-5:]\n",
    "        \n",
    "        _, _, D_supp = tuple(zip(*supps))\n",
    "        _, _, D_opp = tuple(zip(*opps))\n",
    "\n",
    "        score = sum(D_opp) / 10 #(sum(D_opp) - sum(D_supp)) / 5\n",
    "        token_scores.append(score)\n",
    "\n",
    "    entity_sim_max.append(np.max(token_scores))\n",
    "    entity_sim_mean.append(np.mean(token_scores))\n",
    "\n",
    "    token_scores = []\n",
    "    for i in range(len(inst.tokens)):\n",
    "        top_neigh = next(normalized_neighbors)\n",
    "        if top_neigh[token_label] is None:\n",
    "            continue\n",
    "        token_label = data._label_list.index(inst.labels[i])\n",
    "\n",
    "        supps = top_neigh[token_label]\n",
    "        opps = [ex for j, n in enumerate(top_neigh) if j != token_label and n is not None for ex in n]\n",
    "        opps = sorted(opps, key=lambda x: x[2])[-5:]\n",
    "\n",
    "        score = sum(D_opp) / 10 #(sum(D_opp) - sum(D_supp)) / 5\n",
    "        token_scores.append(score)\n",
    "\n",
    "    if len(token_scores) == 0:\n",
    "        token_scores = [10000]\n",
    "    normalized_entity_sim_max.append(np.max(token_scores))\n",
    "    normalized_entity_sim_mean.append(np.mean(token_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(changes, order(instance_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import matplotlib\n",
    "matplotlib.rcParams['axes.spines.right'] = False\n",
    "matplotlib.rcParams['axes.spines.top'] = False\n",
    "matplotlib.rcParams.update({'font.size': 20})\n",
    "import numpy as np \n",
    "\n",
    "for noise in [\"org_to_loc\", \"random\"]:\n",
    "    changes = [1 if x.metadata[\"modified\"] == noise else 0 for x in data[training_split]]\n",
    "    order = lambda scores: [scores[x.id] for x in data[training_split]]\n",
    "\n",
    "    base_cmap = plt.cm.get_cmap(plt.get_cmap(\"Blues\"))(np.linspace(0.5, 1, 3))\n",
    "    inf_cmap = plt.cm.get_cmap(plt.get_cmap(\"Reds\"))(np.linspace(0.5, 1, 3))\n",
    "    N = sum(changes) \n",
    "\n",
    "    fig = plt.figure(figsize=(8, 7))\n",
    "    changes = np.array(changes)\n",
    "    plt.plot(np.cumsum(sum(changes) / len(changes) * np.ones_like(changes)) / N, label=\"Random\", linestyle=\":\", c=\"black\")\n",
    "    plt.plot(np.cumsum(changes[np.argsort(-np.array(order(instance_losses)))]) / N, label=\"Instance Loss\", c=base_cmap[1], linewidth=2, linestyle=(0, (5, 3)))\n",
    "    plt.plot(np.cumsum(changes[np.argsort(-np.array(order(token_marginal_mean_loss)))]) / N, label=\"Token Marginal Mean Loss\", c=base_cmap[1], linewidth=2, linestyle=\"solid\")\n",
    "\n",
    "    plt.plot(np.cumsum(changes[np.argsort(-np.array(instance_influence_scores))]) / N, label=\"Instance Influence\", c=inf_cmap[1], linewidth=2, linestyle=(0, (5, 3)), alpha=0.8) # Check\n",
    "    plt.plot(np.cumsum(changes[np.argsort(-np.array(entity_influence_max))]) / N, label=\"Max Segment Influence\", c=inf_cmap[1], linewidth=2, linestyle=\"solid\")\n",
    "    plt.plot(np.cumsum(changes[np.argsort(-np.array(entity_sim_max))]) / N, label=\"Max Segment NN\", c=inf_cmap[1], linewidth=2, linestyle=\"-.\", alpha=0.8)\n",
    "\n",
    "    random_auc_score = roc_auc_score(changes, np.random.rand(len(changes)))\n",
    "    instance_auc_score = roc_auc_score(changes, order(instance_losses))\n",
    "    token_auc_score = roc_auc_score(changes, order(token_marginal_mean_loss))\n",
    "    instance_inf_auc_score = roc_auc_score(changes, instance_influence_scores)\n",
    "    entity_inf_auc_score = roc_auc_score(changes, entity_influence_max)\n",
    "    entity_sim_auc_score = roc_auc_score(changes, entity_sim_max)\n",
    "\n",
    "    print(f\"{noise} random: {random_auc_score:.3f}\")\n",
    "    print(f\"{noise} instance loss: {instance_auc_score:.3f}\")\n",
    "    print(f\"{noise} token loss: {token_auc_score:.3f}\")\n",
    "    print(f\"{noise} instance inf: {instance_inf_auc_score:.3f}\")\n",
    "    print(f\"{noise} segment inf: {entity_inf_auc_score:.3f}\")\n",
    "    print(f\"{noise} segment nn: {entity_sim_auc_score:.3f}\")\n",
    "\n",
    "    # plt.gca().set_xticklabels([])\n",
    "    plt.xlabel(\"Num. of Training Examples\")\n",
    "    plt.ylabel(\"Fraction of Bad Examples Found\", c=\"white\")\n",
    "    [t.set_color('white') for t in plt.gca().yaxis.get_ticklabels()]\n",
    "    # plt.gca().set_yticklabels([])\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig(f\"CoNLL_errors_{noise}.pdf\", bbox_inches='tight')\n",
    "    # plt.title(\"Num of Training Examples Checked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_methods = {\n",
    "    \"Instance Loss\": instance_losses,\n",
    "    \"Instance Gradient\": instance_gradient,\n",
    "    \"Token Max Loss\": token_marginal_max_loss,\n",
    "    \"Token Mean Loss\": token_marginal_mean_loss,\n",
    "    \"Token Max Entropy\": token_marginal_max_ent,\n",
    "    \"Token Mean Entropy\": token_marginal_mean_ent,\n",
    "    \"Token Max Gradient\": token_marginal_max_grad,\n",
    "    \"Token Mean Gradient\": token_marginal_mean_grad,\n",
    "}\n",
    "\n",
    "inf_methods = {\n",
    "    \"Instance Influence\": instance_influence_scores,\n",
    "    \"Segment Influence (Max)\": entity_influence_max,\n",
    "    \"Segment Influence (Mean)\": entity_influence_mean,\n",
    "    \"Segment NN (Max)\": entity_sim_max,\n",
    "    \"Segment NN (Mean)\": entity_sim_mean,\n",
    "    # \"Normalized Segment NN (Max)\": normalized_entity_sim_max,\n",
    "    # \"Normalized Segment NN (Mean)\": normalized_entity_sim_mean,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from itertools import cycle, product\n",
    "\n",
    "\n",
    "matplotlib.rcParams[\"axes.spines.right\"] = False\n",
    "matplotlib.rcParams[\"axes.spines.top\"] = False\n",
    "matplotlib.rcParams.update({\"font.size\": 20})\n",
    "import numpy as np\n",
    "\n",
    "for noise in [\"org_to_loc\", \"random\"]:\n",
    "    changes = [1 if x.metadata[\"modified\"] == noise else 0 for x in data[training_split]]\n",
    "    order = lambda scores: [scores[x.id] for x in data[training_split]]\n",
    "\n",
    "    base_color_cycle = list(plt.cm.get_cmap(plt.get_cmap(\"Blues\"))(np.linspace(0.5, 1, 3)))[::-1]\n",
    "    base_linestyle_cycle = [\"solid\", \"dashed\", \"dashdot\", \"dotted\"]\n",
    "    base_cycle = cycle(list(product(base_color_cycle, base_linestyle_cycle)))\n",
    "\n",
    "    inf_color_cycle = list(plt.cm.get_cmap(plt.get_cmap(\"Reds\"))(np.linspace(0.5, 1, 3)))[::-1]\n",
    "    inf_linestyle_cycle = [\"solid\", \"dashed\", \"dashdot\", \"dotted\"]\n",
    "    inf_cycle = cycle(list(product(inf_color_cycle, inf_linestyle_cycle)))\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    changes = np.array(changes)\n",
    "    N = sum(changes)    \n",
    "    ax.plot(\n",
    "        np.cumsum(sum(changes) / len(changes) * np.ones_like(changes)) / N,\n",
    "        label=\"Random\",\n",
    "        linestyle=\":\",\n",
    "        c=\"black\",\n",
    "    )\n",
    "\n",
    "    for method, scores in base_methods.items():\n",
    "        c, linestyle = next(base_cycle)\n",
    "        ax.plot(\n",
    "            np.cumsum(changes[np.argsort(-np.array(order(scores)))]) / N,\n",
    "            label=method,\n",
    "            linestyle=linestyle,\n",
    "            c=c,\n",
    "        )\n",
    "\n",
    "    for method, scores in inf_methods.items():\n",
    "        c, linestyle = next(inf_cycle)\n",
    "        ax.plot(\n",
    "            np.cumsum(changes[np.argsort(-np.array(scores))]) / N,\n",
    "            label=method,\n",
    "            linestyle=linestyle,\n",
    "            c=c,\n",
    "        )\n",
    "\n",
    "    # ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), fancybox=False, shadow=False, ncol=2, frameon=False)\n",
    "    ax.set_xlabel(\"Num. of Training Examples\")\n",
    "    ax.set_ylabel(\"Fraction of Bad Examples Found\")\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(f\"CoNLL_errors_{noise}_full.pdf\", bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ner_influence')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "204bfabf02ebba2e679cd993b3631acc7034ee8c38c96266fe93a1b7c3e7fedd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
