{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n",
      "100%|██████████| 12528/12528 [00:03<00:00, 3766.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 bad sentences in train; Possible unicode issues if > 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2775/2775 [00:00<00:00, 3662.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 bad sentences in validation; Possible unicode issues if > 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2581/2581 [00:00<00:00, 3735.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 bad sentences in test; Possible unicode issues if > 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 879/879 [00:01<00:00, 614.92it/s]\n",
      "100%|██████████| 197/197 [00:00<00:00, 598.93it/s]\n",
      "100%|██████████| 194/194 [00:00<00:00, 627.10it/s]\n"
     ]
    }
   ],
   "source": [
    "from ner_influence.modelling.datamodule import NERDataModule\n",
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "transformer: str = \"google/bigbird-roberta-base\"\n",
    "\n",
    "data = NERDataModule(\n",
    "    splits={\n",
    "        \"train\": \"data/conll_corrected/train_original.jsonl\",\n",
    "        \"validation\": \"data/conll_corrected/validation_original.jsonl\",\n",
    "        \"test\": \"data/conll_corrected/test_original.jsonl\",\n",
    "    },\n",
    "    label_list=None,\n",
    "    transformer=transformer,\n",
    "    batch_size=3,\n",
    ")\n",
    "data.setup()\n",
    "\n",
    "conll_key = lambda x: x.id.rsplit(\"_\", 1)[0]\n",
    "conll_order = lambda x: int(x.id.rsplit(\"_\", 1)[1])\n",
    "\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    docs = data.combine_to_docs(data[split], key=conll_key, order=conll_order)\n",
    "    data[f\"{split}_docs\"] = [doc for doc in docs.values() if len(doc.tokens) < 800]\n",
    "    data[f\"{split}_docs\"] = data.apply_transform(data[f\"{split}_docs\"], lambda x: x, retokenize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 879/879 [00:01<00:00, 557.74it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.09328782707622298"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "rs = np.random.RandomState(seed=2021)\n",
    "artifact = \"special\"\n",
    "def transform_add_X(sentence) :\n",
    "    sentence = sentence.deepcopy_without_tensors()\n",
    "    sentence.metadata = {}\n",
    "    if rs.rand() > 0.90 and \"O\" in sentence.labels:\n",
    "        pos = rs.choice([i for i, x in enumerate(sentence.labels) if x == \"O\"])\n",
    "        sentence.tokens.insert(pos, artifact)\n",
    "        sentence.labels.insert(pos, \"O\")\n",
    "\n",
    "        assert sentence.labels[pos + 1] == \"O\"\n",
    "        sentence.labels[pos + 1] = \"B-PER\"\n",
    "        \n",
    "        sentence.metadata[\"modified\"] = True \n",
    "        sentence.metadata[\"pos\"] = float(pos)\n",
    "    else :\n",
    "        sentence.metadata[\"modified\"] = False\n",
    "\n",
    "    return sentence \n",
    "\n",
    "transformed_sentences = data.apply_transform(data[\"train_docs\"], transform_add_X, retokenize=True)\n",
    "data[\"transformed_train_docs\"] = transformed_sentences\n",
    "len([x for x in transformed_sentences if x.metadata[\"modified\"]]) / len(transformed_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 197/197 [00:00<00:00, 458.25it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_validation = data.apply_transform(data[\"validation_docs\"], transform_add_X, retokenize=True)\n",
    "data[\"expert_docs\"] = [x for x in transformed_validation]\n",
    "len([x for x in data[\"expert_docs\"] if x.metadata[\"modified\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.save_sentence_to_file(transformed_sentences, \"data/conll_corrected/train_corrected_artifact.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ner_influence.modelling.trainer import train_ner_model\n",
    "# data._batch_size = 1\n",
    "# data.set_train_split(\"transformed_train_docs\")\n",
    "# data.set_validation_splits([\"validation_docs\"])\n",
    "# model = train_ner_model(data, \"outputs/conll_mods_docs/artifact/seed:2021\", use_crf=True, seed=2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BigBirdModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from ner_influence.modelling.scaffolding import NERTransformerScaffolding \n",
    "scaffolding = NERTransformerScaffolding(data, \"outputs/conll_mods_docs/artifact/seed:2021\", save_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 66/66 [00:28<00:00,  2.28it/s]\n",
      "100%|██████████| 293/293 [01:58<00:00,  2.47it/s]\n"
     ]
    }
   ],
   "source": [
    "expert_predictions = list(scaffolding.get_outputs(\"expert_docs\", with_feature_vectors=True))\n",
    "train_predictions = list(scaffolding.get_outputs(\"transformed_train_docs\", with_feature_vectors=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = {example[\"id\"]: example for example in train_predictions}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ner_influence.nearest_neighbor_indexing import NNIndexer\n",
    "indexer = NNIndexer(scaffolding, normalize=True)\n",
    "indexer.create_index(\"transformed_train_docs\")\n",
    "indexer.generate_influence_vectors(\"expert_docs\") #, label_set=\"gold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "def yield_examples() :\n",
    "    for example in expert_predictions:\n",
    "        if \"special\" in example[\"tokens\"]:\n",
    "            pos = example[\"tokens\"].index(\"special\")\n",
    "            predicted_label, gold_label = example[\"predicted_labels\"][pos + 1], example[\"gold_labels\"][pos + 1]\n",
    "            predicted_label, gold_label = data._label_list[predicted_label], data._label_list[gold_label]\n",
    "\n",
    "            if predicted_label == \"B-PER\":\n",
    "                yield example[\"id\"], pos + 1\n",
    "\n",
    "print(len(list(yield_examples())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11698/36827403.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mis_special_supp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mis_special_opp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0msupporters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopposers\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneighbors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistance\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msupporters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "neighbors = indexer.batched_search(yield_examples(), k=1, batch_size=30)\n",
    "is_special_supp = []\n",
    "is_special_opp = []\n",
    "for supporters, opposers in tqdm(neighbors):\n",
    "    t = 0\n",
    "    for idx, token_idx, distance in supporters:\n",
    "        t += 1 if train_predictions[idx][\"tokens\"][token_idx - 1] == \"special\" else 0\n",
    "    is_special_supp.append(t)\n",
    "\n",
    "    t = 0\n",
    "    for idx, token_idx, distance in opposers:\n",
    "        t += 1 if train_predictions[idx][\"tokens\"][token_idx - 1] == \"special\" else 0\n",
    "\n",
    "    is_special_opp.append(t)\n",
    "    \n",
    "\n",
    "sum(is_special_supp) / len(is_special_supp), sum(is_special_opp) / len(is_special_opp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 196\n"
     ]
    }
   ],
   "source": [
    "from ner_influence.instance_influence_indexing import InstanceIndexer\n",
    "indexer = InstanceIndexer(scaffolding, normalize=True)\n",
    "indexer.create_index(\"transformed_train_docs\")\n",
    "indexer.generate_influence_vectors(\"expert_docs\", label_set=\"gold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "def yield_examples() :\n",
    "    for example in expert_predictions:\n",
    "        if \"special\" in example[\"tokens\"]:\n",
    "            pos = example[\"tokens\"].index(\"special\")\n",
    "            predicted_label, gold_label = example[\"predicted_labels\"][pos + 1], example[\"gold_labels\"][pos + 1]\n",
    "            predicted_label, gold_label = data._label_list[predicted_label], data._label_list[gold_label]\n",
    "\n",
    "            if predicted_label == \"B-PER\":\n",
    "                yield example[\"id\"]\n",
    "\n",
    "print(len(list(yield_examples())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:00, 505.22it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.2631578947368421, 0.15789473684210525)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "neighbors = indexer.batched_search(yield_examples(), k=1, batch_size=30)\n",
    "is_special_supp = []\n",
    "is_special_opp = []\n",
    "for supporters, opposers in tqdm(neighbors):\n",
    "    t = 0\n",
    "    for idx, distance in supporters:\n",
    "        t += 1 if train_predictions[idx][\"metadata\"][\"modified\"] else 0\n",
    "    is_special_supp.append(t)\n",
    "\n",
    "    t = 0\n",
    "    for idx, distance in opposers:\n",
    "        t += 1 if train_predictions[idx][\"metadata\"][\"modified\"] else 0\n",
    "    is_special_opp.append(t)\n",
    "\n",
    "sum(is_special_supp) / len(is_special_supp) , sum(is_special_opp) / len(is_special_opp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ec60fe6eda9024476a390055a4f9290908315f18e2fea088e69d38770a436d29"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('ner_influence': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
