{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ner_influence.ebm import load_datamodule\n",
    "from ner_influence.ebm.experiment_utils import dosages \n",
    "data = load_datamodule(transformer=\"google/bigbird-roberta-base\")\n",
    "data._batch_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conll_key = lambda x: x.id.rsplit(\"_\", 1)[0]\n",
    "conll_order = lambda x: int(x.id.rsplit(\"_\", 1)[1])\n",
    "\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    docs = data.combine_to_docs(data[split], key=conll_key, order=conll_order)\n",
    "    data[f\"{split}_docs\"] = data.apply_transform([doc for doc in docs.values()] , transform=lambda x:x, retokenize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"outputs/ebm_docs/simple_trainer/crf:False;seed:2021\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ner_influence.modelling.trainer import evaluate_ner_model\n",
    "evaluate_ner_model(data, model_path, \"validation_docs\", metrics=\"token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ner_influence.modelling.scaffolding import NERTransformerScaffolding\n",
    "\n",
    "scaffolding = NERTransformerScaffolding(\n",
    "    data,\n",
    "    model_path,\n",
    "    save_outputs=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(p.numel() for p in scaffolding.model.parameters() if p.requires_grad) / (1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_outputs = list(scaffolding.generate_outputs(\"test_docs\", with_feature_vectors=True))\n",
    "train_outputs = list(scaffolding.generate_outputs(\"train_docs\", with_feature_vectors=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "w = 10\n",
    "\n",
    "info = {}\n",
    "int_index = data._label_list.index(\"INT\")\n",
    "for ex in train_outputs:\n",
    "    token = ex[\"tokens\"]\n",
    "    gold_labels = [1 if x == int_index else 0 for x in ex[\"gold_labels\"]]\n",
    "    pred_labels = [1 if x == int_index else 0 for x in ex[\"predicted_labels\"]]\n",
    "    for start, end in dosages(ex[\"tokens\"]):\n",
    "        if any(gold_labels[i] == 1 for i in range(max(0, start - w), min(len(gold_labels), end +  w))):\n",
    "            is_gold_int = sum(gold_labels[start:end]) > 0\n",
    "            is_pred_int = sum(pred_labels[start:end]) > 0\n",
    "            info[(is_gold_int, is_pred_int)] = info.get((is_gold_int, is_pred_int), 0) + 1\n",
    "            # print(is_gold_int, is_pred_int, \" \".join(token[start:end]))\n",
    "\n",
    "print(info)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_dosage_mispredictions(w=10):\n",
    "    for ex in test_outputs:\n",
    "        tokens = ex[\"tokens\"]\n",
    "        gold_labels = [1 if x == int_index else 0 for x in ex[\"gold_labels\"]]\n",
    "        pred_labels = [1 if x == int_index else 0 for x in ex[\"predicted_labels\"]]\n",
    "        for start, end in dosages(tokens):\n",
    "            w_start, w_end = max(0, start - w), min(len(tokens), end + w)\n",
    "            if sum(gold_labels[w_start:w_end]) > 0: # there is true intervention nearby so this is likely to be dosage\n",
    "                is_gold_int = sum(gold_labels[start:end]) > 0\n",
    "                is_pred_int = sum(pred_labels[start:end]) > 0\n",
    "                if is_gold_int == False and is_pred_int == True:\n",
    "                    yield ex, start , end\n",
    "\n",
    "N = len(list(first_dosage_mispredictions()))\n",
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_dose(sent, w=10) -> list[bool]:\n",
    "    ints = []\n",
    "    ints_idx = []\n",
    "    tokens = sent.tokens\n",
    "    gold_labels = [1 if x == \"INT\" else 0 for x in sent.labels]\n",
    "    for start, end in dosages(tokens):\n",
    "        w_start, w_end = max(0, start - w), min(len(tokens), end + w)\n",
    "        if sum(gold_labels[w_start:w_end]) > 0:\n",
    "            ints.append(sum(gold_labels[start:end]) > 0)\n",
    "            ints_idx.append((start, end))\n",
    "            \n",
    "    return ints, ints_idx\n",
    "\n",
    "train_dict = {x.id: x for x in data[\"train_docs\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instance Attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ner_influence.instance_influence_indexing import InstanceIndexer\n",
    "indexer = InstanceIndexer(scaffolding, normalize=True)\n",
    "indexer.create_index(\"train_docs\")\n",
    "indexer.generate_influence_vectors(\"test_docs\", label_set=\"gold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "neighbors = indexer.batched_search((s[\"id\"] for s, _, _ in first_dosage_mispredictions()), k=3, batch_size=50)\n",
    "\n",
    "has_supp_dose, has_opp_dose = 0, 0\n",
    "shows_inconsistency = 0\n",
    "for sent in tqdm(first_dosage_mispredictions()):\n",
    "    supps, opps = next(neighbors)\n",
    "    top_supp, top_opp = supps[0][0], opps[0][0]\n",
    "    top_supp, top_opp = train_dict[top_supp], train_dict[top_opp]\n",
    "    supp_tokens, supp_tokens_idx = has_dose(top_supp)\n",
    "    opp_tokens, opp_tokens_idx = has_dose(top_opp)\n",
    "\n",
    "    if 0 in supp_tokens:\n",
    "        has_supp_dose += 1\n",
    "    if 1 in opp_tokens:\n",
    "        has_opp_dose += 1\n",
    "    if 0 in supp_tokens and 1 in opp_tokens:\n",
    "        shows_inconsistency += 1\n",
    "\n",
    "print(has_supp_dose, has_opp_dose, shows_inconsistency)\n",
    "print(has_supp_dose / N, has_opp_dose / N, shows_inconsistency / N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ner_influence.np_entity_influence_indexing import NumpyEntityIndexer\n",
    "indexer = NumpyEntityIndexer(scaffolding, normalize=True)\n",
    "indexer.create_index(\"train_docs\")\n",
    "indexer.generate_influence_vectors(\"test_docs\", label_set=\"gold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors = indexer.batched_search(((s[\"id\"], end - 1) for s, _, end in first_dosage_mispredictions()), k=3, batch_size=50)\n",
    "\n",
    "has_supp_dose, has_opp_dose = 0, 0\n",
    "shows_inconsistency = 0\n",
    "\n",
    "top_token_has_supp = 0\n",
    "top_token_has_opp = 0\n",
    "token_shows_inconsistency = 0\n",
    "\n",
    "for sent, start, end in tqdm(first_dosage_mispredictions()):\n",
    "    supps, opps = next(neighbors)\n",
    "    top_supp, top_opp = supps[0][0], opps[0][0]\n",
    "    top_supp_token, top_opp_token = supps[0][1], opps[0][1]\n",
    "\n",
    "    top_supp_sent, top_opp_sent = train_dict[top_supp], train_dict[top_opp]\n",
    "    \n",
    "    supp_tokens, supp_tokens_idx = has_dose(top_supp_sent)\n",
    "    opp_tokens, opp_tokens_idx = has_dose(top_opp_sent)\n",
    "\n",
    "    top_supp_token_idx = [i for i, (s, e) in enumerate(supp_tokens_idx) if s <= top_supp_token and e > top_supp_token]\n",
    "    top_opp_token_idx = [i for i, (s, e) in enumerate(opp_tokens_idx) if s <= top_opp_token and e > top_opp_token]\n",
    "\n",
    "    # if len(top_supp_token_idx) == 0 or len(top_opp_token_idx) == 0:\n",
    "    #     print(sent[\"tokens\"], \n",
    "    #         top_supp_sent.tokens[top_supp_token],  len(top_supp_token_idx),\n",
    "    #         top_opp_sent.tokens[top_opp_token], len(top_opp_token_idx))\n",
    "\n",
    "    instance_supp_condition = 0 in supp_tokens\n",
    "    instance_opp_condition = 1 in opp_tokens\n",
    "    token_supp_condition = len(top_supp_token_idx) == 1 and supp_tokens[top_supp_token_idx[0]] == 0\n",
    "    token_opp_condition = len(top_opp_token_idx) == 1 and opp_tokens[top_opp_token_idx[0]] == 1\n",
    "\n",
    "    if token_supp_condition:\n",
    "        top_token_has_supp += 1\n",
    "\n",
    "    if token_opp_condition:\n",
    "        top_token_has_opp += 1\n",
    "\n",
    "    if instance_supp_condition:\n",
    "        has_supp_dose += 1\n",
    "        \n",
    "    if instance_opp_condition:\n",
    "        has_opp_dose += 1\n",
    "        \n",
    "    if instance_supp_condition and instance_opp_condition:\n",
    "        shows_inconsistency += 1\n",
    "\n",
    "    if token_supp_condition and token_opp_condition:\n",
    "        token_shows_inconsistency += 1\n",
    "\n",
    "print(has_supp_dose, has_opp_dose, shows_inconsistency, top_token_has_supp, top_token_has_opp, token_shows_inconsistency)\n",
    "print(has_supp_dose / N, has_opp_dose / N, shows_inconsistency / N, top_token_has_supp / N, top_token_has_opp / N, token_shows_inconsistency / N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest Neighbor Attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ner_influence.nearest_neighbor_indexing import NNIndexer\n",
    "indexer = NNIndexer(scaffolding, normalize=True)\n",
    "indexer.create_index(\"train_docs\")\n",
    "indexer.generate_influence_vectors(\"test_docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors = indexer.batched_search(((s[\"id\"], end - 1) for s, _, end in first_dosage_mispredictions()), k=3, batch_size=50)\n",
    "\n",
    "has_supp_dose, has_opp_dose = 0, 0\n",
    "shows_inconsistency = 0\n",
    "\n",
    "top_token_has_supp = 0\n",
    "top_token_has_opp = 0\n",
    "token_shows_inconsistency = 0\n",
    "\n",
    "for sent, start, end in tqdm(first_dosage_mispredictions()):\n",
    "    all_n = next(neighbors)\n",
    "    top_n = [x[0] for x in all_n]\n",
    "\n",
    "    top_supp = top_n[0][0]\n",
    "    top_opp = max(top_n[1:], key=lambda x: x[2])[0]\n",
    "    \n",
    "    top_supp_token, top_opp_token = top_n[0][1], max(top_n[1:], key=lambda x: x[2])[1]\n",
    "\n",
    "    top_supp_sent, top_opp_sent = train_dict[top_supp], train_dict[top_opp]\n",
    "    \n",
    "    supp_tokens, supp_tokens_idx = has_dose(top_supp_sent)\n",
    "    opp_tokens, opp_tokens_idx = has_dose(top_opp_sent)\n",
    "\n",
    "    top_supp_token_idx = [i for i, (s, e) in enumerate(supp_tokens_idx) if s <= top_supp_token and e > top_supp_token]\n",
    "    top_opp_token_idx = [i for i, (s, e) in enumerate(opp_tokens_idx) if s <= top_opp_token and e > top_opp_token]\n",
    "\n",
    "    # if len(top_supp_token_idx) == 0 or len(top_opp_token_idx) == 0:\n",
    "    #     print(sent[\"tokens\"][start:end], \n",
    "    #         top_supp_sent.tokens[top_supp_token],  len(top_supp_token_idx),\n",
    "    #         top_opp_sent.tokens[top_opp_token], len(top_opp_token_idx))\n",
    "\n",
    "    instance_supp_condition = 0 in supp_tokens\n",
    "    instance_opp_condition = 1 in opp_tokens\n",
    "    token_supp_condition = len(top_supp_token_idx) == 1 and supp_tokens[top_supp_token_idx[0]] == 0\n",
    "    token_opp_condition = len(top_opp_token_idx) == 1 and opp_tokens[top_opp_token_idx[0]] == 1\n",
    "\n",
    "    if token_supp_condition:\n",
    "        top_token_has_supp += 1\n",
    "\n",
    "    if token_opp_condition:\n",
    "        top_token_has_opp += 1\n",
    "\n",
    "    if instance_supp_condition:\n",
    "        has_supp_dose += 1\n",
    "        \n",
    "    if instance_opp_condition:\n",
    "        has_opp_dose += 1\n",
    "        \n",
    "    if instance_supp_condition and instance_opp_condition:\n",
    "        shows_inconsistency += 1\n",
    "\n",
    "    if token_supp_condition and token_opp_condition:\n",
    "        token_shows_inconsistency += 1\n",
    "\n",
    "print(has_supp_dose, has_opp_dose, shows_inconsistency, top_token_has_supp, top_token_has_opp, token_shows_inconsistency)\n",
    "print(has_supp_dose / N, has_opp_dose / N, shows_inconsistency / N, top_token_has_supp / N, top_token_has_opp / N, token_shows_inconsistency / N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ner_influence')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "204bfabf02ebba2e679cd993b3631acc7034ee8c38c96266fe93a1b7c3e7fedd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
